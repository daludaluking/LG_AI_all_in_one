{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length=2\n",
    "x_data_dim=4\n",
    "batch_size=100\n",
    "normalization_flag=True\n",
    "\n",
    "data_dir = '../dataset'\n",
    "fname = os.path.join(data_dir, 'data-02-stock_daily.csv')\n",
    "df = pd.read_csv(fname)\n",
    "dataset=df.copy()\n",
    "ori_Y=dataset.pop(\"Close\")\n",
    "ori_X=dataset.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(ori_X,ori_Y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, Y_train, Y_val= train_test_split(X_train,Y_train, test_size=0.2, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 데이터의 min , max, mean, std 값 구하기.\n",
    "dataset_stats = X_train.describe()\n",
    "dataset_stats = dataset_stats.transpose()\n",
    "y_dataset_stats = Y_train.describe()\n",
    "y_dataset_stats = y_dataset_stats.transpose()\n",
    "## data normalization\n",
    "def min_max_norm(x):\n",
    "  return (x - dataset_stats['min']) / (dataset_stats['max'] - dataset_stats['min'])\n",
    "\n",
    "def standard_norm(x):\n",
    "  return (x - dataset_stats['mean']) / dataset_stats['std']\n",
    "\n",
    "def y_standard_norm(x):\n",
    "  return (x - y_dataset_stats['mean']) / y_dataset_stats['std']\n",
    "\n",
    "def rever_y_standard_norm(x):\n",
    "  return x  * y_dataset_stats['std'] + y_dataset_stats['mean']\n",
    "\n",
    "if normalization_flag==True:\n",
    "    standard_norm_train_data = standard_norm(X_train)\n",
    "    standard_norm_val_data = standard_norm(X_val)\n",
    "    standard_norm_test_data = standard_norm(X_test)\n",
    "\n",
    "    standard_norm_Y_train_data = y_standard_norm(Y_train)\n",
    "    standard_norm_Y_val_data = y_standard_norm(Y_val)\n",
    "    standard_norm_Y_test_data = y_standard_norm(Y_test)\n",
    "\n",
    "    data_gen_train=tf.keras.preprocessing.sequence.TimeseriesGenerator(standard_norm_train_data.values.tolist(), standard_norm_Y_train_data.values.tolist(),\n",
    "                                                                        length=seq_length, sampling_rate=1,\n",
    "                                                                        batch_size=batch_size)\n",
    "    data_gen_val=tf.keras.preprocessing.sequence.TimeseriesGenerator(standard_norm_val_data.values.tolist(), standard_norm_Y_val_data.values.tolist(),\n",
    "                                                                       length=seq_length, sampling_rate=1,\n",
    "                                                                       batch_size=batch_size)\n",
    "    data_gen_test=tf.keras.preprocessing.sequence.TimeseriesGenerator(standard_norm_test_data.values.tolist(), standard_norm_Y_test_data.values.tolist(),\n",
    "                                                                       length=seq_length, sampling_rate=1,\n",
    "                                                                       batch_size=batch_size)\n",
    "else:\n",
    "    data_gen_train = tf.keras.preprocessing.sequence.TimeseriesGenerator(X_train.values.tolist(),Y_train.values.tolist(),\n",
    "                                                                   length=seq_length, sampling_rate=1,\n",
    "                                                                   batch_size=batch_size)\n",
    "    data_gen_val = tf.keras.preprocessing.sequence.TimeseriesGenerator(X_val.values.tolist(),Y_val.values.tolist(),\n",
    "                                                                   length=seq_length, sampling_rate=1,\n",
    "                                                                   batch_size=batch_size)\n",
    "    data_gen_test = tf.keras.preprocessing.sequence.TimeseriesGenerator(X_test.values.tolist(),Y_test.values.tolist(),\n",
    "                                                                        length=seq_length, sampling_rate=1,\n",
    "                                                                        batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_Layer = tf.keras.layers.Input(shape=(seq_length, x_data_dim))\n",
    "x=tf.keras.layers.GRU(20,activation='tanh')(input_Layer) ##GRU\n",
    "x=tf.keras.layers.Dense(20,activation='relu')(x)\n",
    "x=tf.keras.layers.Dense(10,activation='relu')(x)\n",
    "Out_Layer=tf.keras.layers.Dense(1,activation=None)(x)\n",
    "model = tf.keras.Model(inputs=[input_Layer], outputs=[Out_Layer])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "loss_function=tf.keras.losses.mean_squared_error\n",
    "optimize=tf.keras.optimizers.Adam(learning_rate=0.0009)\n",
    "metric=tf.keras.metrics.mean_absolute_error\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimize,\n",
    "              metrics=[metric])\n",
    "\n",
    "history = model.fit(\n",
    "      data_gen_train,\n",
    "      validation_data=data_gen_val,\n",
    "      steps_per_epoch=len(X_train)//batch_size,\n",
    "      epochs=1000,\n",
    "    validation_freq=1\n",
    ")\n",
    "print(model.evaluate(data_gen_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_X, test_data_Y=data_gen_test[0]\n",
    "prediction_Y=model.predict(test_data_X).flatten()\n",
    "Y_test=rever_y_standard_norm(test_data_Y.flatten())\n",
    "\n",
    "visual_y=[]\n",
    "visual_pre_y=[]\n",
    "for i in range(len(prediction_Y)):\n",
    "    label = Y_test[i]\n",
    "    prediction = prediction_Y[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))\n",
    "    visual_y.append(label)\n",
    "    visual_pre_y.append(prediction)\n",
    "\n",
    "time = range(1, len(visual_y) + 1)\n",
    "plt.plot(time, visual_y, 'r', label='ture')\n",
    "plt.plot(time, visual_pre_y, 'b', label='prediction')\n",
    "plt.title('stock prediction')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
