{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MNIST 데이터 불러오기\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "## X_train shape : ( 이미지 장수, 이미지 가로 , 이미지 세로 )\n",
    "## Y_train : (정답 갯수, ) - > [ 0 ,1, 2, 4 ,5, 5, ....  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## X_train shape : ( 이미지 장수, 가로 , 세로 ) 이차원 이어서 Neural Network(=Multi Layer Perceptron ) 입력 차원인 1차원으로\n",
    "## 변경 해주어야 합니다.\n",
    "## reshape 사용하여 차원 변형\n",
    "X_train = X_train.reshape(X_train.shape[0], 784).astype('float32')  ## 28x28= 784\n",
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##  [ 0 ,1, 2, 4 ,5, 5, ....  ] -> [ [1 0 0 0 0 0 0 0], [0 1 0 0 0 0 0.. ] , ] 원핫인코딩\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "Y_test =  tf.keras.utils.to_categorical(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#모델 설계\n",
    "# method 1\n",
    "input_Layer = tf.keras.layers.Input(shape=(784,)) ## 데이터 입력\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(input_Layer) # hidden layer1\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "## out은 task를 수행.\n",
    "## 10개의 class를 분류해야힘\n",
    "## 분류해야하는 class  0~ 9: 10개 그래서 아웃풋 perceptron 10개\n",
    "## 각각의 perceptron이 출력값을 가지면 -> 상관관계가 없음.\n",
    "## 출력을 확률 처럼 맵핑 하고자 softmax 를 사용하면\n",
    "# 출력이 확률처럼나오게 된다. 때문에 멀티 클레스피케이션에서 softmax를 쓰는게 일반적.\n",
    "Out_Layer= tf.keras.layers.Dense(10, activation='softmax')(x)  ## 분류해야하는 class  0~ 9: 10개 그래서 아웃풋 perceptron 10개\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_Layer], outputs=[Out_Layer])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 17.4850 - categorical_accuracy: 0.7674\n",
      "Epoch 00001: val_loss improved from inf to 1.32068, saving model to ./MNIST_model\\01-1.3207.hdf5\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 15.1579 - categorical_accuracy: 0.7876 - val_loss: 1.3207 - val_categorical_accuracy: 0.9152\n",
      "Epoch 2/30\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.8262 - categorical_accuracy: 0.9267\n",
      "Epoch 00002: val_loss improved from 1.32068 to 0.76919, saving model to ./MNIST_model\\02-0.7692.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.8251 - categorical_accuracy: 0.9268 - val_loss: 0.7692 - val_categorical_accuracy: 0.9312\n",
      "Epoch 3/30\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.4063 - categorical_accuracy: 0.9507\n",
      "Epoch 00003: val_loss improved from 0.76919 to 0.60139, saving model to ./MNIST_model\\03-0.6014.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3961 - categorical_accuracy: 0.9512 - val_loss: 0.6014 - val_categorical_accuracy: 0.9388\n",
      "Epoch 4/30\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.2067 - categorical_accuracy: 0.9674\n",
      "Epoch 00004: val_loss improved from 0.60139 to 0.55675, saving model to ./MNIST_model\\04-0.5567.hdf5\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.2098 - categorical_accuracy: 0.9670 - val_loss: 0.5567 - val_categorical_accuracy: 0.9415\n",
      "Epoch 5/30\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1189 - categorical_accuracy: 0.9779\n",
      "Epoch 00005: val_loss improved from 0.55675 to 0.50093, saving model to ./MNIST_model\\05-0.5009.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1182 - categorical_accuracy: 0.9779 - val_loss: 0.5009 - val_categorical_accuracy: 0.9470\n",
      "Epoch 6/30\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0596 - categorical_accuracy: 0.9867\n",
      "Epoch 00006: val_loss improved from 0.50093 to 0.48201, saving model to ./MNIST_model\\06-0.4820.hdf5\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.0597 - categorical_accuracy: 0.9864 - val_loss: 0.4820 - val_categorical_accuracy: 0.9473\n",
      "Epoch 7/30\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0284 - categorical_accuracy: 0.9929\n",
      "Epoch 00007: val_loss improved from 0.48201 to 0.47514, saving model to ./MNIST_model\\07-0.4751.hdf5\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.0286 - categorical_accuracy: 0.9929 - val_loss: 0.4751 - val_categorical_accuracy: 0.9484\n",
      "Epoch 8/30\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0138 - categorical_accuracy: 0.9967\n",
      "Epoch 00008: val_loss improved from 0.47514 to 0.46988, saving model to ./MNIST_model\\08-0.4699.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0136 - categorical_accuracy: 0.9967 - val_loss: 0.4699 - val_categorical_accuracy: 0.9488\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0065 - categorical_accuracy: 0.9984\n",
      "Epoch 00009: val_loss improved from 0.46988 to 0.45523, saving model to ./MNIST_model\\09-0.4552.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0065 - categorical_accuracy: 0.9984 - val_loss: 0.4552 - val_categorical_accuracy: 0.9507\n",
      "Epoch 10/30\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0033 - categorical_accuracy: 0.9996\n",
      "Epoch 00010: val_loss improved from 0.45523 to 0.45138, saving model to ./MNIST_model\\10-0.4514.hdf5\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.0031 - categorical_accuracy: 0.9996 - val_loss: 0.4514 - val_categorical_accuracy: 0.9508\n",
      "Epoch 11/30\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0012 - categorical_accuracy: 0.9999\n",
      "Epoch 00011: val_loss did not improve from 0.45138\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0012 - categorical_accuracy: 0.9999 - val_loss: 0.4527 - val_categorical_accuracy: 0.9512\n",
      "Epoch 12/30\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 6.7979e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.45138\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.8955e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4515 - val_categorical_accuracy: 0.9512\n",
      "Epoch 13/30\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 5.3683e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.45138\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.3768e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4516 - val_categorical_accuracy: 0.9514\n",
      "Epoch 14/30\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 4.5680e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.45138\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 4.6438e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4515 - val_categorical_accuracy: 0.9514\n",
      "Epoch 15/30\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 4.1267e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00015: val_loss improved from 0.45138 to 0.45115, saving model to ./MNIST_model\\15-0.4512.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 4.1460e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4512 - val_categorical_accuracy: 0.9517\n",
      "Epoch 16/30\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 3.7284e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00016: val_loss improved from 0.45115 to 0.45022, saving model to ./MNIST_model\\16-0.4502.hdf5\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 3.7279e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4502 - val_categorical_accuracy: 0.9523\n",
      "Epoch 17/30\n",
      "47/48 [============================>.] - ETA: 0s - loss: 3.4344e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 3.4244e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4508 - val_categorical_accuracy: 0.9520\n",
      "Epoch 18/30\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 3.1365e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 3.1355e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4507 - val_categorical_accuracy: 0.9523\n",
      "Epoch 19/30\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 2.8874e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 2.8948e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4503 - val_categorical_accuracy: 0.9523\n",
      "Epoch 20/30\n",
      "47/48 [============================>.] - ETA: 0s - loss: 2.7044e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 2.7033e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4503 - val_categorical_accuracy: 0.9523\n",
      "Epoch 21/30\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 2.4923e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5234e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4507 - val_categorical_accuracy: 0.9522\n",
      "Epoch 22/30\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 2.3128e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.3570e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4511 - val_categorical_accuracy: 0.9523\n",
      "Epoch 23/30\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 2.2012e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.2133e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4508 - val_categorical_accuracy: 0.9523\n",
      "Epoch 24/30\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 2.0914e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.0882e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4511 - val_categorical_accuracy: 0.9523\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/48 [===========================>..] - ETA: 0s - loss: 1.9738e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9775e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4507 - val_categorical_accuracy: 0.9528\n",
      "Epoch 26/30\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 1.8551e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.45022\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.8569e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4506 - val_categorical_accuracy: 0.9530\n",
      "dict_keys(['loss', 'categorical_accuracy', 'val_loss', 'val_categorical_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 컴파일\n",
    "loss=tf.keras.losses.categorical_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "metric=tf.keras.metrics.categorical_accuracy\n",
    "model.compile(loss=loss,\n",
    "             optimizer=optimizer,\n",
    "             metrics=[metric])\n",
    "\n",
    "# 베스트 모델 저장을 위한 디렉토리 선언\n",
    "MODEL_DIR = './MNIST_model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "## 저장한 모델의 주소와 이름.\n",
    "modelpath=\"./MNIST_model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "## 사용할 callback 함수 선언.\n",
    "callback_list=[tf.keras.callbacks.ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "               tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    "\n",
    "# 모델의 실행\n",
    "#  validation_data 옵션으로 테스트 데이터만 넣어주어서 검증 데이터 분류가 가능\n",
    "result = model.fit(X_train, Y_train, validation_split=0.2, epochs=30, batch_size=1000, verbose=1, callbacks=callback_list)\n",
    "print(result.history.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))\n",
    "# 테스트 셋의 오차\n",
    "val_loss = result.history['val_loss']\n",
    "\n",
    "# 학습셋의 오차\n",
    "loss = result.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(val_loss))\n",
    "plt.plot(x_len, val_loss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "# plt.axis([0, 20, 0, 0.35])\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
