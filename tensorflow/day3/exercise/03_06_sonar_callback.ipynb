{"nbformat":4,"nbformat_minor":2,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit ('base': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"03_06_sonar_callback.ipynb","provenance":[]},"interpreter":{"hash":"14c878ca9e09cb855c5aba830c5d56400199f035e888de3a1c3895cb2a489872"}},"cells":[{"cell_type":"code","execution_count":1,"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import math\n","import pandas as pd\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    # Currently, memory growth needs to be the same across GPUs\n","    for gpu in gpus:\n","      tf.config.experimental.set_memory_growth(gpu, True)\n","    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","  except RuntimeError as e:\n","    # Memory growth must be set before GPUs have been initialized\n","    print(e)"],"outputs":[{"output_type":"stream","name":"stdout","text":["1 Physical GPUs, 1 Logical GPUs\n"]}],"metadata":{"id":"UrG2w62s2mWw","executionInfo":{"status":"ok","timestamp":1629216229636,"user_tz":-540,"elapsed":3024,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}}}},{"cell_type":"code","execution_count":2,"source":["\n","def step_decay(epoch):\n","\tinitial_lrate = 0.1\n","\tdrop = 0.5\n","\tepochs_drop = 10.0\n","\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n","\treturn lrate\n"],"outputs":[],"metadata":{"id":"CsU32uU82p-H","executionInfo":{"status":"ok","timestamp":1629216229638,"user_tz":-540,"elapsed":11,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}}}},{"cell_type":"code","execution_count":9,"source":["\n","# 데이터 입력\n","df = pd.read_csv('../dataset/sonar.csv')\n","# 데이터 분류\n","dataset = df.values\n","X = dataset[:,0:60].astype(float)\n","Y_obj = dataset[:,60]\n","\n","# 문자열을 숫자로 변환\n","e = LabelEncoder()\n","e.fit(Y_obj)\n","Y = e.transform(Y_obj)\n","print(X)\n","print(Y)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n"," [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n"," [0.01   0.0171 0.0623 ... 0.0044 0.004  0.0117]\n"," ...\n"," [0.0522 0.0437 0.018  ... 0.0138 0.0077 0.0031]\n"," [0.0303 0.0353 0.049  ... 0.0079 0.0036 0.0048]\n"," [0.026  0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"KFX28-OP2rhe","executionInfo":{"status":"error","timestamp":1629216234408,"user_tz":-540,"elapsed":685,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}},"outputId":"4f8191e6-6637-4abd-9079-3d7da15aa55b"}},{"cell_type":"code","execution_count":11,"source":["\n","# 전체 데이터에서 학습 데이터와 테스트 데이터로 구분\n","X_train1, X_test, Y_train1, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=Y)  ## shuffle=True로 하면 데이터를 섞어서 나눔\n","## 학습 셋에서 학습과 검증 데이터로 구분\n","X_train, X_valid, Y_train, Y_valid = train_test_split(X_train1, Y_train1, test_size=0.2, shuffle=True, stratify=Y_train1)  ## shuffle=True로 하면 데이터를 섞어서 나눔\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"wpgw7ukW2tdO","executionInfo":{"status":"error","timestamp":1629216239272,"user_tz":-540,"elapsed":276,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}},"outputId":"44fd9b22-cb9b-4010-d93e-75545e7d124e"}},{"cell_type":"code","execution_count":12,"source":["\n","# 모델의 설정\n","activation=tf.keras.activations.sigmoid\n","input_Layer = tf.keras.layers.Input(shape=(60,))\n","x = tf.keras.layers.Dense(120, activation=activation)(input_Layer)\n","x = tf.keras.layers.Dense(100, activation=activation)(x)\n","x = tf.keras.layers.Dense(50, activation=activation)(x)\n","Out_Layer= tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","model = tf.keras.models.Model(inputs=[input_Layer], outputs=[Out_Layer])\n","model.summary()\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 60)]              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 120)               7320      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 100)               12100     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 50)                5050      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 51        \n","=================================================================\n","Total params: 24,521\n","Trainable params: 24,521\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgj__ok42uv7","executionInfo":{"status":"ok","timestamp":1629216247780,"user_tz":-540,"elapsed":432,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}},"outputId":"38154c43-b390-496a-f561-a8598f46680f"}},{"cell_type":"code","execution_count":16,"source":["\n","\n","# 모델 컴파일\n","loss=tf.keras.losses.binary_crossentropy\n","optimizer = tf.keras.optimizers.SGD(lr=0.01)\n","metrics=tf.keras.metrics.binary_accuracy\n","\n","modelpath=\"./sonar_best_model/{epoch:02d}-{val_loss:.4f}.h5\"\n","c1 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n","c2 = tf.keras.callbacks.ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n","c3 = tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1)\n","clabacks_list =[c1, c2, c3]\n","\n","model.compile(loss=loss,\n","             optimizer=optimizer,\n","             metrics=[metrics])\n","\n","## model fit은 histoy를 반환한다. 훈련중의 발생하는 모든 정보를 담고 있는 딕셔너리.\n","result=model.fit(X_train, Y_train, epochs=300, batch_size=50, validation_data=(X_valid,Y_valid), callbacks=clabacks_list) # validation_data=(X_valid,Y_valid)을 추가하여 학습시 검증을 해줌.\n","## histoy는 딕셔너리이므로 keys()를 통해 출력의 key(카테고리)를 확인하여 무엇을 받고 있는지 확인.\n","print(result.history.keys())\n"],"outputs":[{"output_type":"stream","name":"stderr","text":["/home/dalu/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","\n","Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 1s 106ms/step - loss: 1.9894 - binary_accuracy: 0.5303 - val_loss: 0.9345 - val_binary_accuracy: 0.5455\n","\n","Epoch 00001: val_loss improved from inf to 0.93445, saving model to ./sonar_best_model/01-0.9345.h5\n","Epoch 2/300\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.8035 - binary_accuracy: 0.4848 - val_loss: 0.7479 - val_binary_accuracy: 0.4545\n","\n","Epoch 00002: val_loss improved from 0.93445 to 0.74789, saving model to ./sonar_best_model/02-0.7479.h5\n","Epoch 3/300\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.7383 - binary_accuracy: 0.4697 - val_loss: 0.7362 - val_binary_accuracy: 0.4545\n","\n","Epoch 00003: val_loss improved from 0.74789 to 0.73622, saving model to ./sonar_best_model/03-0.7362.h5\n","Epoch 4/300\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.7188 - binary_accuracy: 0.4697 - val_loss: 0.7023 - val_binary_accuracy: 0.4545\n","\n","Epoch 00004: val_loss improved from 0.73622 to 0.70228, saving model to ./sonar_best_model/04-0.7023.h5\n","Epoch 5/300\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6963 - binary_accuracy: 0.4697 - val_loss: 0.6900 - val_binary_accuracy: 0.5455\n","\n","Epoch 00005: val_loss improved from 0.70228 to 0.68999, saving model to ./sonar_best_model/05-0.6900.h5\n","Epoch 6/300\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6969 - binary_accuracy: 0.5303 - val_loss: 0.6997 - val_binary_accuracy: 0.5455\n","\n","Epoch 00006: val_loss did not improve from 0.68999\n","Epoch 7/300\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.7067 - binary_accuracy: 0.5303 - val_loss: 0.6952 - val_binary_accuracy: 0.5455\n","\n","Epoch 00007: val_loss did not improve from 0.68999\n","Epoch 8/300\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6982 - binary_accuracy: 0.5303 - val_loss: 0.6891 - val_binary_accuracy: 0.5455\n","\n","Epoch 00008: val_loss improved from 0.68999 to 0.68909, saving model to ./sonar_best_model/08-0.6891.h5\n","Epoch 9/300\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6936 - binary_accuracy: 0.5303 - val_loss: 0.6940 - val_binary_accuracy: 0.4545\n","\n","Epoch 00009: val_loss did not improve from 0.68909\n","Epoch 10/300\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6943 - binary_accuracy: 0.4697 - val_loss: 0.6954 - val_binary_accuracy: 0.4545\n","\n","Epoch 00010: val_loss did not improve from 0.68909\n","Epoch 11/300\n","\n","Epoch 00011: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6946 - binary_accuracy: 0.4697 - val_loss: 0.6935 - val_binary_accuracy: 0.4545\n","\n","Epoch 00011: val_loss did not improve from 0.68909\n","Epoch 12/300\n","\n","Epoch 00012: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6927 - binary_accuracy: 0.5455 - val_loss: 0.6902 - val_binary_accuracy: 0.5455\n","\n","Epoch 00012: val_loss did not improve from 0.68909\n","Epoch 13/300\n","\n","Epoch 00013: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 22ms/step - loss: 0.6921 - binary_accuracy: 0.5303 - val_loss: 0.6890 - val_binary_accuracy: 0.5455\n","\n","Epoch 00013: val_loss improved from 0.68909 to 0.68901, saving model to ./sonar_best_model/13-0.6890.h5\n","Epoch 14/300\n","\n","Epoch 00014: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6922 - binary_accuracy: 0.5303 - val_loss: 0.6891 - val_binary_accuracy: 0.5455\n","\n","Epoch 00014: val_loss did not improve from 0.68901\n","Epoch 15/300\n","\n","Epoch 00015: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6924 - binary_accuracy: 0.5303 - val_loss: 0.6890 - val_binary_accuracy: 0.5455\n","\n","Epoch 00015: val_loss did not improve from 0.68901\n","Epoch 16/300\n","\n","Epoch 00016: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6924 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00016: val_loss did not improve from 0.68901\n","Epoch 17/300\n","\n","Epoch 00017: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6895 - val_binary_accuracy: 0.5455\n","\n","Epoch 00017: val_loss did not improve from 0.68901\n","Epoch 18/300\n","\n","Epoch 00018: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6896 - val_binary_accuracy: 0.5455\n","\n","Epoch 00018: val_loss did not improve from 0.68901\n","Epoch 19/300\n","\n","Epoch 00019: LearningRateScheduler reducing learning rate to 0.05.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6897 - val_binary_accuracy: 0.5455\n","\n","Epoch 00019: val_loss did not improve from 0.68901\n","Epoch 20/300\n","\n","Epoch 00020: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6917 - binary_accuracy: 0.5303 - val_loss: 0.6897 - val_binary_accuracy: 0.5455\n","\n","Epoch 00020: val_loss did not improve from 0.68901\n","Epoch 21/300\n","\n","Epoch 00021: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6916 - binary_accuracy: 0.5303 - val_loss: 0.6899 - val_binary_accuracy: 0.5455\n","\n","Epoch 00021: val_loss did not improve from 0.68901\n","Epoch 22/300\n","\n","Epoch 00022: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6900 - val_binary_accuracy: 0.5455\n","\n","Epoch 00022: val_loss did not improve from 0.68901\n","Epoch 23/300\n","\n","Epoch 00023: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6902 - val_binary_accuracy: 0.5455\n","\n","Epoch 00023: val_loss did not improve from 0.68901\n","Epoch 24/300\n","\n","Epoch 00024: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6900 - val_binary_accuracy: 0.5455\n","\n","Epoch 00024: val_loss did not improve from 0.68901\n","Epoch 25/300\n","\n","Epoch 00025: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6917 - binary_accuracy: 0.5303 - val_loss: 0.6897 - val_binary_accuracy: 0.5455\n","\n","Epoch 00025: val_loss did not improve from 0.68901\n","Epoch 26/300\n","\n","Epoch 00026: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6891 - val_binary_accuracy: 0.5455\n","\n","Epoch 00026: val_loss did not improve from 0.68901\n","Epoch 27/300\n","\n","Epoch 00027: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6890 - val_binary_accuracy: 0.5455\n","\n","Epoch 00027: val_loss improved from 0.68901 to 0.68901, saving model to ./sonar_best_model/27-0.6890.h5\n","Epoch 28/300\n","\n","Epoch 00028: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6918 - binary_accuracy: 0.5303 - val_loss: 0.6890 - val_binary_accuracy: 0.5455\n","\n","Epoch 00028: val_loss did not improve from 0.68901\n","Epoch 29/300\n","\n","Epoch 00029: LearningRateScheduler reducing learning rate to 0.025.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6919 - binary_accuracy: 0.5303 - val_loss: 0.6890 - val_binary_accuracy: 0.5455\n","\n","Epoch 00029: val_loss did not improve from 0.68901\n","Epoch 30/300\n","\n","Epoch 00030: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6917 - binary_accuracy: 0.5303 - val_loss: 0.6891 - val_binary_accuracy: 0.5455\n","\n","Epoch 00030: val_loss did not improve from 0.68901\n","Epoch 31/300\n","\n","Epoch 00031: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6921 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00031: val_loss did not improve from 0.68901\n","Epoch 32/300\n","\n","Epoch 00032: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00032: val_loss did not improve from 0.68901\n","Epoch 33/300\n","\n","Epoch 00033: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00033: val_loss did not improve from 0.68901\n","Epoch 34/300\n","\n","Epoch 00034: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00034: val_loss did not improve from 0.68901\n","Epoch 35/300\n","\n","Epoch 00035: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6896 - val_binary_accuracy: 0.5455\n","\n","Epoch 00035: val_loss did not improve from 0.68901\n","Epoch 36/300\n","\n","Epoch 00036: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6920 - binary_accuracy: 0.5303 - val_loss: 0.6899 - val_binary_accuracy: 0.5455\n","\n","Epoch 00036: val_loss did not improve from 0.68901\n","Epoch 37/300\n","\n","Epoch 00037: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6896 - val_binary_accuracy: 0.5455\n","\n","Epoch 00037: val_loss did not improve from 0.68901\n","Epoch 38/300\n","\n","Epoch 00038: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00038: val_loss did not improve from 0.68901\n","Epoch 39/300\n","\n","Epoch 00039: LearningRateScheduler reducing learning rate to 0.0125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00039: val_loss did not improve from 0.68901\n","Epoch 40/300\n","\n","Epoch 00040: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6916 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00040: val_loss did not improve from 0.68901\n","Epoch 41/300\n","\n","Epoch 00041: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00041: val_loss did not improve from 0.68901\n","Epoch 42/300\n","\n","Epoch 00042: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00042: val_loss did not improve from 0.68901\n","Epoch 43/300\n","\n","Epoch 00043: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00043: val_loss did not improve from 0.68901\n","Epoch 44/300\n","\n","Epoch 00044: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00044: val_loss did not improve from 0.68901\n","Epoch 45/300\n","\n","Epoch 00045: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00045: val_loss did not improve from 0.68901\n","Epoch 46/300\n","\n","Epoch 00046: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 22ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00046: val_loss did not improve from 0.68901\n","Epoch 47/300\n","\n","Epoch 00047: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00047: val_loss did not improve from 0.68901\n","Epoch 48/300\n","\n","Epoch 00048: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 28ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00048: val_loss did not improve from 0.68901\n","Epoch 49/300\n","\n","Epoch 00049: LearningRateScheduler reducing learning rate to 0.00625.\n","3/3 [==============================] - 0s 25ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00049: val_loss did not improve from 0.68901\n","Epoch 50/300\n","\n","Epoch 00050: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00050: val_loss did not improve from 0.68901\n","Epoch 51/300\n","\n","Epoch 00051: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00051: val_loss did not improve from 0.68901\n","Epoch 52/300\n","\n","Epoch 00052: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 22ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00052: val_loss did not improve from 0.68901\n","Epoch 53/300\n","\n","Epoch 00053: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00053: val_loss did not improve from 0.68901\n","Epoch 54/300\n","\n","Epoch 00054: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00054: val_loss did not improve from 0.68901\n","Epoch 55/300\n","\n","Epoch 00055: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00055: val_loss did not improve from 0.68901\n","Epoch 56/300\n","\n","Epoch 00056: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00056: val_loss did not improve from 0.68901\n","Epoch 57/300\n","\n","Epoch 00057: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 13ms/step - loss: 0.6915 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00057: val_loss did not improve from 0.68901\n","Epoch 58/300\n","\n","Epoch 00058: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00058: val_loss did not improve from 0.68901\n","Epoch 59/300\n","\n","Epoch 00059: LearningRateScheduler reducing learning rate to 0.003125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00059: val_loss did not improve from 0.68901\n","Epoch 60/300\n","\n","Epoch 00060: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00060: val_loss did not improve from 0.68901\n","Epoch 61/300\n","\n","Epoch 00061: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00061: val_loss did not improve from 0.68901\n","Epoch 62/300\n","\n","Epoch 00062: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 13ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6892 - val_binary_accuracy: 0.5455\n","\n","Epoch 00062: val_loss did not improve from 0.68901\n","Epoch 63/300\n","\n","Epoch 00063: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 13ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00063: val_loss did not improve from 0.68901\n","Epoch 64/300\n","\n","Epoch 00064: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00064: val_loss did not improve from 0.68901\n","Epoch 65/300\n","\n","Epoch 00065: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00065: val_loss did not improve from 0.68901\n","Epoch 66/300\n","\n","Epoch 00066: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 22ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00066: val_loss did not improve from 0.68901\n","Epoch 67/300\n","\n","Epoch 00067: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00067: val_loss did not improve from 0.68901\n","Epoch 68/300\n","\n","Epoch 00068: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 23ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00068: val_loss did not improve from 0.68901\n","Epoch 69/300\n","\n","Epoch 00069: LearningRateScheduler reducing learning rate to 0.0015625.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00069: val_loss did not improve from 0.68901\n","Epoch 70/300\n","\n","Epoch 00070: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00070: val_loss did not improve from 0.68901\n","Epoch 71/300\n","\n","Epoch 00071: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00071: val_loss did not improve from 0.68901\n","Epoch 72/300\n","\n","Epoch 00072: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00072: val_loss did not improve from 0.68901\n","Epoch 73/300\n","\n","Epoch 00073: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00073: val_loss did not improve from 0.68901\n","Epoch 74/300\n","\n","Epoch 00074: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 24ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00074: val_loss did not improve from 0.68901\n","Epoch 75/300\n","\n","Epoch 00075: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00075: val_loss did not improve from 0.68901\n","Epoch 76/300\n","\n","Epoch 00076: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6914 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00076: val_loss did not improve from 0.68901\n","Epoch 77/300\n","\n","Epoch 00077: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6893 - val_binary_accuracy: 0.5455\n","\n","Epoch 00077: val_loss did not improve from 0.68901\n","Epoch 78/300\n","\n","Epoch 00078: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00078: val_loss did not improve from 0.68901\n","Epoch 79/300\n","\n","Epoch 00079: LearningRateScheduler reducing learning rate to 0.00078125.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00079: val_loss did not improve from 0.68901\n","Epoch 80/300\n","\n","Epoch 00080: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00080: val_loss did not improve from 0.68901\n","Epoch 81/300\n","\n","Epoch 00081: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00081: val_loss did not improve from 0.68901\n","Epoch 82/300\n","\n","Epoch 00082: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00082: val_loss did not improve from 0.68901\n","Epoch 83/300\n","\n","Epoch 00083: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00083: val_loss did not improve from 0.68901\n","Epoch 84/300\n","\n","Epoch 00084: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00084: val_loss did not improve from 0.68901\n","Epoch 85/300\n","\n","Epoch 00085: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00085: val_loss did not improve from 0.68901\n","Epoch 86/300\n","\n","Epoch 00086: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00086: val_loss did not improve from 0.68901\n","Epoch 87/300\n","\n","Epoch 00087: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00087: val_loss did not improve from 0.68901\n","Epoch 88/300\n","\n","Epoch 00088: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00088: val_loss did not improve from 0.68901\n","Epoch 89/300\n","\n","Epoch 00089: LearningRateScheduler reducing learning rate to 0.000390625.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00089: val_loss did not improve from 0.68901\n","Epoch 90/300\n","\n","Epoch 00090: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00090: val_loss did not improve from 0.68901\n","Epoch 91/300\n","\n","Epoch 00091: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00091: val_loss did not improve from 0.68901\n","Epoch 92/300\n","\n","Epoch 00092: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00092: val_loss did not improve from 0.68901\n","Epoch 93/300\n","\n","Epoch 00093: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00093: val_loss did not improve from 0.68901\n","Epoch 94/300\n","\n","Epoch 00094: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00094: val_loss did not improve from 0.68901\n","Epoch 95/300\n","\n","Epoch 00095: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00095: val_loss did not improve from 0.68901\n","Epoch 96/300\n","\n","Epoch 00096: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00096: val_loss did not improve from 0.68901\n","Epoch 97/300\n","\n","Epoch 00097: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00097: val_loss did not improve from 0.68901\n","Epoch 98/300\n","\n","Epoch 00098: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00098: val_loss did not improve from 0.68901\n","Epoch 99/300\n","\n","Epoch 00099: LearningRateScheduler reducing learning rate to 0.0001953125.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00099: val_loss did not improve from 0.68901\n","Epoch 100/300\n","\n","Epoch 00100: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00100: val_loss did not improve from 0.68901\n","Epoch 101/300\n","\n","Epoch 00101: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00101: val_loss did not improve from 0.68901\n","Epoch 102/300\n","\n","Epoch 00102: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00102: val_loss did not improve from 0.68901\n","Epoch 103/300\n","\n","Epoch 00103: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00103: val_loss did not improve from 0.68901\n","Epoch 104/300\n","\n","Epoch 00104: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00104: val_loss did not improve from 0.68901\n","Epoch 105/300\n","\n","Epoch 00105: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 14ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00105: val_loss did not improve from 0.68901\n","Epoch 106/300\n","\n","Epoch 00106: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 13ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00106: val_loss did not improve from 0.68901\n","Epoch 107/300\n","\n","Epoch 00107: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00107: val_loss did not improve from 0.68901\n","Epoch 108/300\n","\n","Epoch 00108: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 13ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00108: val_loss did not improve from 0.68901\n","Epoch 109/300\n","\n","Epoch 00109: LearningRateScheduler reducing learning rate to 9.765625e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00109: val_loss did not improve from 0.68901\n","Epoch 110/300\n","\n","Epoch 00110: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00110: val_loss did not improve from 0.68901\n","Epoch 111/300\n","\n","Epoch 00111: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00111: val_loss did not improve from 0.68901\n","Epoch 112/300\n","\n","Epoch 00112: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00112: val_loss did not improve from 0.68901\n","Epoch 113/300\n","\n","Epoch 00113: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00113: val_loss did not improve from 0.68901\n","Epoch 114/300\n","\n","Epoch 00114: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 21ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00114: val_loss did not improve from 0.68901\n","Epoch 115/300\n","\n","Epoch 00115: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00115: val_loss did not improve from 0.68901\n","Epoch 116/300\n","\n","Epoch 00116: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00116: val_loss did not improve from 0.68901\n","Epoch 117/300\n","\n","Epoch 00117: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00117: val_loss did not improve from 0.68901\n","Epoch 118/300\n","\n","Epoch 00118: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00118: val_loss did not improve from 0.68901\n","Epoch 119/300\n","\n","Epoch 00119: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00119: val_loss did not improve from 0.68901\n","Epoch 120/300\n","\n","Epoch 00120: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 20ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00120: val_loss did not improve from 0.68901\n","Epoch 121/300\n","\n","Epoch 00121: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00121: val_loss did not improve from 0.68901\n","Epoch 122/300\n","\n","Epoch 00122: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00122: val_loss did not improve from 0.68901\n","Epoch 123/300\n","\n","Epoch 00123: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00123: val_loss did not improve from 0.68901\n","Epoch 124/300\n","\n","Epoch 00124: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 18ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00124: val_loss did not improve from 0.68901\n","Epoch 125/300\n","\n","Epoch 00125: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00125: val_loss did not improve from 0.68901\n","Epoch 126/300\n","\n","Epoch 00126: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 19ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00126: val_loss did not improve from 0.68901\n","Epoch 127/300\n","\n","Epoch 00127: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n","3/3 [==============================] - 0s 16ms/step - loss: 0.6913 - binary_accuracy: 0.5303 - val_loss: 0.6894 - val_binary_accuracy: 0.5455\n","\n","Epoch 00127: val_loss did not improve from 0.68901\n","Epoch 00127: early stopping\n","dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy', 'lr'])\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"1QngpPUn2wxs","executionInfo":{"status":"error","timestamp":1629216270789,"user_tz":-540,"elapsed":403,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}},"outputId":"42c0fd81-59cd-40d8-a8ac-440ae8af4318"}},{"cell_type":"code","execution_count":7,"source":["\n","### result에서 loss와 val_loss의 key를 가지는 값들만 추출\n","loss = result.history['loss']\n","val_loss = result.history['val_loss']\n","### loss와 val_loss를 그래프화\n","epochs = range(1, len(loss) + 1)\n","plt.subplot(211)  ## 2x1 개의 그래프 중에 1번째\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","### result에서 binary_accuracy와 val_binary_accuracy key를 가지는 값들만 추출\n","acc = result.history['binary_accuracy']\n","val_acc = result.history['val_binary_accuracy']\n","\n","### binary_accuracy와 val_binary_accuracy key를 그래프화\n","plt.subplot(212)  ## 2x1 개의 그래프 중에 2번째\n","plt.plot(epochs, acc, 'ro', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","## 그래프 띄우기\n","plt.show()\n","\n","\n","\n"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b0e101f5e87a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### result에서 loss와 val_loss의 key를 가지는 값들만 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### loss와 val_loss를 그래프화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"W__UtZO422Yi","executionInfo":{"status":"error","timestamp":1629216277054,"user_tz":-540,"elapsed":266,"user":{"displayName":"김현우","photoUrl":"","userId":"06560543018646300359"}},"outputId":"332ce7f8-6832-4b78-da88-38e516897f75"}},{"cell_type":"code","execution_count":null,"source":["# model.evalueate를 통해 테스트 데이터로 정확도 확인하기.\n","## model.evaluate(X_test, Y_test)의 리턴값은 [loss, binary_acuuracy ]  -> 위 model.compile에서 metrics=[ keras.metrics.binary_accuracy]옵션을 주어서 binary acuuracy 출력됨.\n","print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))\n","\n"],"outputs":[],"metadata":{"id":"kkRhOv4323-b"}}]}